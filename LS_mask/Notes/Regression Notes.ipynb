{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Ridge Regression Derivations\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "### 1. Linear Regression Model\n",
    "\n",
    "The linear regression model with \\( p \\) predictors is given by:\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon $$\n",
    "\n",
    "where:\n",
    "- \\( Y \\) is the vector of observed values.\n",
    "- \\( X \\) is the design matrix (including predictor variables).\n",
    "- \\( \\beta \\) is the vector of coefficients to be estimated.\n",
    "- \\( \\epsilon \\) is the vector of residuals (errors).\n",
    "\n",
    "### 2. Least Squares Objective\n",
    "\n",
    "The goal is to minimize the sum of squared residuals:\n",
    "\n",
    "$$ \\text{Minimize} \\; \\| Y - X \\beta \\|^2 $$\n",
    "\n",
    "Expanding the residuals:\n",
    "\n",
    "$$ \\| Y - X \\beta \\|^2 = (Y - X \\beta)^T (Y - X \\beta) $$\n",
    "\n",
    "Expanding this:\n",
    "\n",
    "$$ (Y - X \\beta)^T (Y - X \\beta) = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta $$\n",
    "\n",
    "### 3. Finding the Minimum\n",
    "\n",
    "To find the minimum, take the derivative with respect to \\( \\beta \\) and set it to zero:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left[ Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta \\right] = 0 $$\n",
    "\n",
    "Simplify the derivative:\n",
    "\n",
    "$$ -2 X^T Y + 2 X^T X \\beta = 0 $$\n",
    "\n",
    "Divide by 2:\n",
    "\n",
    "$$ X^T X \\beta = X^T Y $$\n",
    "\n",
    "### 4. Solving for (\\beta)\n",
    "\n",
    "The normal equation is:\n",
    "\n",
    "$$ \\beta = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "**Note:** For a general case with multiple predictors, the equation is:\n",
    "\n",
    "$$ \\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "where \\(\\beta_0\\) is the intercept and \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients for each predictor variable.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "### 1. Ridge Regression Model\n",
    "\n",
    "Ridge regression adds a regularization term to the linear regression model:\n",
    "\n",
    "$$ \\text{Minimize} \\; \\| Y - X \\beta \\|^2 + \\lambda \\| \\beta \\|^2 $$\n",
    "\n",
    "where:\n",
    "- \\( \\lambda \\) is the regularization parameter.\n",
    "- \\( \\| \\beta \\|^2 \\) is the squared \\( L_2 \\) norm of the coefficient vector \\( \\beta \\).\n",
    "\n",
    "### 2. Expanding the Objective Function\n",
    "\n",
    "Expand the ridge regression objective function:\n",
    "\n",
    "$$ \\text{Objective} = \\| Y - X \\beta \\|^2 + \\lambda \\| \\beta \\|^2 $$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$ \\| Y - X \\beta \\|^2 = (Y - X \\beta)^T (Y - X \\beta) $$\n",
    "\n",
    "$$ \\| \\beta \\|^2 = \\beta^T \\beta $$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$ \\text{Objective} = (Y - X \\beta)^T (Y - X \\beta) + \\lambda \\beta^T \\beta $$\n",
    "\n",
    "Combine quadratic terms:\n",
    "\n",
    "$$ \\text{Objective} = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta + \\lambda \\beta^T \\beta $$\n",
    "\n",
    "$$ = Y^T Y - 2 Y^T X \\beta + \\beta^T (X^T X + \\lambda I) \\beta $$\n",
    "\n",
    "where \\( I \\) is the identity matrix.\n",
    "\n",
    "### 3. Finding the Minimum\n",
    "\n",
    "Take the derivative with respect to \\( \\beta \\) and set it to zero:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left[ Y^T Y - 2 Y^T X \\beta + \\beta^T (X^T X + \\lambda I) \\beta \\right] = 0 $$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$ -2 X^T Y + 2 (X^T X + \\lambda I) \\beta = 0 $$\n",
    "\n",
    "Divide by 2:\n",
    "\n",
    "$$ -X^T Y + (X^T X + \\lambda I) \\beta = 0 $$\n",
    "\n",
    "Rearrange to solve for \\( \\beta \\):\n",
    "\n",
    "$$ (X^T X + \\lambda I) \\beta = X^T Y $$\n",
    "\n",
    "### 4. Solving for \\(\\beta\\)\n",
    "\n",
    "The normal equation for ridge regression is:\n",
    "\n",
    "$$ \\beta = (X^T X + \\lambda I)^{-1} X^T Y $$\n",
    "\n",
    "**Note:** For a general case with multiple predictors, the ridge regression formula is:\n",
    "\n",
    "$$ \\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "where \\(\\beta_0\\) is the intercept and \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients for each predictor variable, adjusted with regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Ridge Regression Derivations\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "### 1. Linear Regression Model\n",
    "\n",
    "The linear regression model with \\( p \\) predictors is given by:\n",
    "\n",
    "$$ Y = X \\beta + \\epsilon $$\n",
    "\n",
    "where:\n",
    "- \\( Y \\) is the vector of observed values.\n",
    "- \\( X \\) is the design matrix (including predictor variables).\n",
    "- \\( \\beta \\) is the vector of coefficients to be estimated.\n",
    "- \\( \\epsilon \\) is the vector of residuals (errors).\n",
    "\n",
    "### 2. Least Squares Objective\n",
    "\n",
    "The goal is to minimize the sum of squared residuals:\n",
    "\n",
    "$$ \\text{Minimize} \\; \\| Y - X \\beta \\|^2 $$\n",
    "\n",
    "Expanding the residuals:\n",
    "\n",
    "$$ \\| Y - X \\beta \\|^2 = (Y - X \\beta)^T (Y - X \\beta) $$\n",
    "\n",
    "Expanding this:\n",
    "\n",
    "$$ (Y - X \\beta)^T (Y - X \\beta) = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta $$\n",
    "\n",
    "### 3. Finding the Minimum\n",
    "\n",
    "To find the minimum, take the derivative with respect to \\( \\beta \\) and set it to zero:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left[ Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta \\right] = 0 $$\n",
    "\n",
    "Simplify the derivative:\n",
    "\n",
    "$$ -2 X^T Y + 2 X^T X \\beta = 0 $$\n",
    "\n",
    "Divide by 2:\n",
    "\n",
    "$$ X^T X \\beta = X^T Y $$\n",
    "\n",
    "### 4. Solving for \\(\\beta\\)\n",
    "\n",
    "The normal equation is:\n",
    "\n",
    "$$ \\beta = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "**Note:** For a general case with multiple predictors, the equation is:\n",
    "\n",
    "$$ \\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "where \\(\\beta_0\\) is the intercept and \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients for each predictor variable.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the following simple linear regression problem with 2 predictors each column a seprate variable:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ Y = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "For this example:\n",
    "- The design matrix \\( X \\) includes 2 predictors (or features) for each of 3 observations.\n",
    "- \\( Y \\) is the vector of target values.\n",
    "\n",
    "To find the coefficients \\( \\beta \\):\n",
    "\n",
    "1. Compute \\( X^T X \\):\n",
    "\n",
    "$$ X^T X = \\begin{bmatrix}\n",
    "1 & 3 & 5 \\\\\n",
    "2 & 4 & 6\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "35 & 44 \\\\\n",
    "44 & 56\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "2. Compute \\( X^T Y \\):\n",
    "\n",
    "$$ X^T Y = \\begin{bmatrix}\n",
    "1 & 3 & 5 \\\\\n",
    "2 & 4 & 6\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "22 \\\\\n",
    "28\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "3. Compute \\( \\beta \\):\n",
    "\n",
    "$$ \\beta = (X^T X)^{-1} X^T Y $$\n",
    "\n",
    "4. Compute \\( (X^T X)^{-1} \\):\n",
    "\n",
    "$$ (X^T X)^{-1} = \\begin{bmatrix}\n",
    "35 & 44 \\\\\n",
    "44 & 56\n",
    "\\end{bmatrix}^{-1} = \\begin{bmatrix}\n",
    "0.5714 & -0.4286 \\\\\n",
    "-0.4286 & 0.3571\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "5. Finally,\n",
    "\n",
    "$$ \\beta = \\begin{bmatrix}\n",
    "0.5714 & -0.4286 \\\\\n",
    "-0.4286 & 0.3571\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "22 \\\\\n",
    "28\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.5\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "### 1. Ridge Regression Model\n",
    "\n",
    "Ridge regression adds a regularization term to the linear regression model:\n",
    "\n",
    "$$ \\text{Minimize} \\; \\| Y - X \\beta \\|^2 + \\lambda \\| \\beta \\|^2 $$\n",
    "\n",
    "where:\n",
    "- \\( \\lambda \\) is the regularization parameter.\n",
    "- \\( \\| \\beta \\|^2 \\) is the squared \\( L_2 \\) norm of the coefficient vector \\( \\beta \\).\n",
    "\n",
    "### 2. Expanding the Objective Function\n",
    "\n",
    "Expand the ridge regression objective function:\n",
    "\n",
    "$$ \\text{Objective} = \\| Y - X \\beta \\|^2 + \\lambda \\| \\beta \\|^2 $$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$ \\| Y - X \\beta \\|^2 = (Y - X \\beta)^T (Y - X \\beta) $$\n",
    "\n",
    "$$ \\| \\beta \\|^2 = \\beta^T \\beta $$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$ \\text{Objective} = (Y - X \\beta)^T (Y - X \\beta) + \\lambda \\beta^T \\beta $$\n",
    "\n",
    "Combine quadratic terms:\n",
    "\n",
    "$$ \\text{Objective} = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta + \\lambda \\beta^T \\beta $$\n",
    "\n",
    "$$ = Y^T Y - 2 Y^T X \\beta + \\beta^T (X^T X + \\lambda I) \\beta $$\n",
    "\n",
    "where \\( I \\) is the identity matrix.\n",
    "\n",
    "### 3. Finding the Minimum\n",
    "\n",
    "Take the derivative with respect to \\( \\beta \\) and set it to zero:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left[ Y^T Y - 2 Y^T X \\beta + \\beta^T (X^T X + \\lambda I) \\beta \\right] = 0 $$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$ -2 X^T Y + 2 (X^T X + \\lambda I) \\beta = 0 $$\n",
    "\n",
    "Divide by 2:\n",
    "\n",
    "$$ -X^T Y + (X^T X + \\lambda I) \\beta = 0 $$\n",
    "\n",
    "Rearrange to solve for \\( \\beta \\):\n",
    "\n",
    "$$ (X^T X + \\lambda I) \\beta = X^T Y $$\n",
    "\n",
    "### 4. Solving for \\(\\beta\\)\n",
    "\n",
    "The normal equation for ridge regression is:\n",
    "\n",
    "$$ \\beta = (X^T X + \\lambda I)^{-1} X^T Y $$\n",
    "\n",
    "**Note:** For a general case with multiple predictors, the ridge regression formula is:\n",
    "\n",
    "$$ \\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "where \\(\\beta_0\\) is the intercept and \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients for each predictor variable, adjusted with regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
