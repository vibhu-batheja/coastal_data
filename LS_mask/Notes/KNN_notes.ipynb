{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are two types of, algorithm KNN regressor and classifier. \n",
    "\n",
    "KNN- finds the neighbours \n",
    "\n",
    "Classifier sees a point based on the point that was closest the KNN algorithm classifies point on it charechterisic like in classifier data is labled and put into a classifier algo and if a new point is enetered it will try to guess based on neighbours which classes does the data belong to.\n",
    "\n",
    "Regressor based on closest point, and its neighbours it tries to guess the value. \n",
    "\n",
    "\n",
    "For KNN also there is a KNN regressor which has a input syntax of following\n",
    "* KNN Regressor Syntax * \n",
    "\n",
    "sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)[source]\n",
    " \n",
    "\n",
    "Main Notes of the following points in the Algo \n",
    "\n",
    "- algorithm is component that decides which neighbours to take or not for distance computation, has 3 options Brute which will compute distance between all the neighbours and KDtree and Balltree (Can read)\n",
    "- metric is distance metric which says what kind of distance will we take between parameters (Euclidian or Minkowski or any other)\n",
    "- weights this decides how weightage to each varaibale you pass to KNN will be Suppose you pass lat, long and elevation what weightage to each component (NEED TO MAKE CUSTOM WEIGHT deciding matrix and try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What is happening with KNN \n",
    "You tried to run the algo for different distance metrics and found that\n",
    "\n",
    "So what is basically happening with KNN is that it takes 2/3/4/5 points based on the distance metric you have provided for eg euclidian so it will calculate lat,log,elev dist, and which ever point has the closest distance metric it uses that point to take average and find an imputed value. \n",
    "\n",
    "Now if you put weights into that while calculating the average the point more closer will be given more weightage than the one far away, (In our case we are getting optimum by just 2 close points so dosnt make much sense to play with weights over here) this is happeneing in the scene. \n",
    "\n",
    "\n",
    "rather then that I would say try to optimise the distance metric. Which would have some dynamic weights, to get the best 2/3/4/5 neighbours. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In conclusion. \n",
    "\n",
    "So in summary, adding more variables would just make the distance calcualtion metric more better but making the distance calculation meetric more accurate would just lead to better selection among the 7-8 afvaialable points. While currently optimal number of points for best KNN MSE is 2. So adding more parameters to just select 2 points would not lead to a significant increase. \n",
    "\n",
    "So maybe later you could try KNN with different variables  and stratergy or weighted KNN. \n",
    "\n",
    "NEED to check iterative imputer still. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now all we can do is try weighted KNN built on own classes later on. also try iterative imputer with other estimators \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted KNN example\n",
    "\n",
    "Below is an example of a simple weighted KNN Algorithm that was built it adjust the weights over 100 runs to optimise the imputation (It makes a class variable called KNN and does KNN in it that class where you can see data fit stores the varaibles and predict computes the distance between all points and does KNN finally ) and these weights are used to check MSE which uses gradient decent to see which direction to move into to get best MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset with features latitude, longitude, elevation, and target value\n",
    "X = np.array([\n",
    "    [35.6895, 139.6917, 44],  # Tokyo\n",
    "    [34.0522, -118.2437, 305], # Los Angeles\n",
    "    [40.7128, -74.0060, 10],  # New York\n",
    "    [51.5074, -0.1278, 35],   # London\n",
    "])\n",
    "y = np.array([100, 200, 300, 150])  # Example target values\n",
    "\n",
    "# Initial weights for the features\n",
    "weights = np.ones(X.shape[1])\n",
    "\n",
    "# Define a custom distance function that uses the weights\n",
    "def weighted_distance(x1, x2, weights):\n",
    "    return np.sqrt(np.sum(weights * (x1 - x2) ** 2))\n",
    "\n",
    "# Define the weighted KNN regressor\n",
    "class WeightedKNNRegressor:\n",
    "    def __init__(self, n_neighbors=3, weights=None):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = np.array([weighted_distance(x, x_train, self.weights) for x_train in self.X_train])\n",
    "            nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "            nearest_values = self.y_train[nearest_indices]\n",
    "            predictions.append(np.mean(nearest_values))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize the weighted KNN regressor\n",
    "weighted_knn = WeightedKNNRegressor(n_neighbors=3, weights=weights)\n",
    "\n",
    "# Simple gradient descent to optimize weights\n",
    "learning_rate = 0.01\n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    weighted_knn.fit(X_train, y_train)\n",
    "    y_pred = weighted_knn.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration}, MSE: {mse}\")\n",
    "\n",
    "    # Compute gradient of weights\n",
    "    gradients = np.zeros_like(weights)\n",
    "    for j in range(len(weights)):\n",
    "        weights[j] += 0.01  # Small perturbation\n",
    "        perturbed_knn = WeightedKNNRegressor(n_neighbors=3, weights=weights)\n",
    "        perturbed_knn.fit(X_train, y_train)\n",
    "        perturbed_pred = perturbed_knn.predict(X_test)\n",
    "        perturbed_mse = mean_squared_error(y_test, perturbed_pred)\n",
    "        gradients[j] = (perturbed_mse - mse) / 0.01\n",
    "        weights[j] -= 0.01  # Revert perturbation\n",
    "\n",
    "    # Update weights\n",
    "    weights -= learning_rate * gradients\n",
    "    print(f\"Updated weights: {weights}\")\n",
    "\n",
    "# Final prediction\n",
    "weighted_knn.fit(X_train, y_train)\n",
    "final_predictions = weighted_knn.predict(X_test)\n",
    "print(f\"Final predictions: {final_predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check IDW as a weighting stratergey in weighted KNN "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
